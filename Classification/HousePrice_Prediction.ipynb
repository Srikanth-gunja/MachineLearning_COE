{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22685ba4-2537-431c-b00b-c502e4a18dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import r2_score,mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "945740c4-1b28-4b82-a410-094623e44157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saich\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 360983887872.0000 - mae: 537398.6250 - val_loss: 373313929216.0000 - val_mae: 543612.8125\n",
      "Epoch 2/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 361956704256.0000 - mae: 532160.1250 - val_loss: 372746485760.0000 - val_mae: 543088.9375\n",
      "Epoch 3/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 351096209408.0000 - mae: 521246.4375 - val_loss: 369553375232.0000 - val_mae: 540134.9375\n",
      "Epoch 4/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 356587339776.0000 - mae: 531694.1250 - val_loss: 359352893440.0000 - val_mae: 530591.5000\n",
      "Epoch 5/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 351656771584.0000 - mae: 523496.2188 - val_loss: 336204038144.0000 - val_mae: 508275.5938\n",
      "Epoch 6/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 311073374208.0000 - mae: 486349.4375 - val_loss: 295039205376.0000 - val_mae: 467512.8438\n",
      "Epoch 7/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 269244678144.0000 - mae: 441994.2500 - val_loss: 235788320768.0000 - val_mae: 409476.0938\n",
      "Epoch 8/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 217744113664.0000 - mae: 388331.2500 - val_loss: 169732521984.0000 - val_mae: 343709.6250\n",
      "Epoch 9/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 153180012544.0000 - mae: 321684.4375 - val_loss: 115909976064.0000 - val_mae: 288438.9062\n",
      "Epoch 10/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 101891407872.0000 - mae: 269222.2500 - val_loss: 88458551296.0000 - val_mae: 257746.9219\n",
      "Epoch 11/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 80810786816.0000 - mae: 244228.4688 - val_loss: 80391233536.0000 - val_mae: 247417.9844\n",
      "Epoch 12/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 72085446656.0000 - mae: 229692.6875 - val_loss: 79103623168.0000 - val_mae: 245378.7812\n",
      "Epoch 13/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 78218133504.0000 - mae: 243049.2031 - val_loss: 78782898176.0000 - val_mae: 244569.6875\n",
      "Epoch 14/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 74563829760.0000 - mae: 235349.1406 - val_loss: 78721261568.0000 - val_mae: 244381.2969\n",
      "Epoch 15/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73158443008.0000 - mae: 234453.4062 - val_loss: 78725390336.0000 - val_mae: 244403.6875\n",
      "Epoch 16/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 76171976704.0000 - mae: 237571.1875 - val_loss: 78594154496.0000 - val_mae: 244031.7656\n",
      "Epoch 17/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75754627072.0000 - mae: 237277.7031 - val_loss: 78605336576.0000 - val_mae: 244139.1406\n",
      "Epoch 18/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 73479503872.0000 - mae: 234252.1562 - val_loss: 78594367488.0000 - val_mae: 244053.1406\n",
      "Epoch 19/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 77142204416.0000 - mae: 241331.2188 - val_loss: 78587838464.0000 - val_mae: 244116.2656\n",
      "Epoch 20/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 80866304000.0000 - mae: 247755.9375 - val_loss: 78536835072.0000 - val_mae: 243988.4219\n",
      "Epoch 21/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74958094336.0000 - mae: 236269.1875 - val_loss: 78543486976.0000 - val_mae: 244016.7812\n",
      "Epoch 22/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 76198969344.0000 - mae: 238524.4844 - val_loss: 78501773312.0000 - val_mae: 243876.5156\n",
      "Epoch 23/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75337121792.0000 - mae: 237385.5625 - val_loss: 78466260992.0000 - val_mae: 243717.5156\n",
      "Epoch 24/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 77314056192.0000 - mae: 240701.5156 - val_loss: 78515527680.0000 - val_mae: 243921.5938\n",
      "Epoch 25/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74947108864.0000 - mae: 235395.9375 - val_loss: 78454669312.0000 - val_mae: 243747.9219\n",
      "Epoch 26/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 73633718272.0000 - mae: 232205.7812 - val_loss: 78376697856.0000 - val_mae: 243423.6875\n",
      "Epoch 27/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74264346624.0000 - mae: 234302.7656 - val_loss: 78436851712.0000 - val_mae: 243668.6875\n",
      "Epoch 28/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75639185408.0000 - mae: 236674.2656 - val_loss: 78450376704.0000 - val_mae: 243665.4219\n",
      "Epoch 29/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 76212494336.0000 - mae: 237939.9844 - val_loss: 78400929792.0000 - val_mae: 243540.7812\n",
      "Epoch 30/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75821129728.0000 - mae: 238208.3125 - val_loss: 78338637824.0000 - val_mae: 243288.9062\n",
      "Epoch 31/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74275913728.0000 - mae: 235122.7656 - val_loss: 78402928640.0000 - val_mae: 243498.1562\n",
      "Epoch 32/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 72822800384.0000 - mae: 231987.2500 - val_loss: 78321762304.0000 - val_mae: 243272.0000\n",
      "Epoch 33/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 78118395904.0000 - mae: 244141.6094 - val_loss: 78427398144.0000 - val_mae: 243623.5938\n",
      "Epoch 34/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75833425920.0000 - mae: 238236.8750 - val_loss: 78400520192.0000 - val_mae: 243589.6875\n",
      "Epoch 35/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75398070272.0000 - mae: 235558.2812 - val_loss: 78336794624.0000 - val_mae: 243331.5781\n",
      "Epoch 36/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74981425152.0000 - mae: 236716.3594 - val_loss: 78340972544.0000 - val_mae: 243350.7188\n",
      "Epoch 37/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75340267520.0000 - mae: 236267.8438 - val_loss: 78347608064.0000 - val_mae: 243353.9531\n",
      "Epoch 38/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74717102080.0000 - mae: 235793.3438 - val_loss: 78321401856.0000 - val_mae: 243232.0781\n",
      "Epoch 39/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 79246049280.0000 - mae: 245627.6875 - val_loss: 78374944768.0000 - val_mae: 243400.3125\n",
      "Epoch 40/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 73086681088.0000 - mae: 232074.9219 - val_loss: 78368489472.0000 - val_mae: 243441.9375\n",
      "Epoch 41/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 76411920384.0000 - mae: 239798.1875 - val_loss: 78371872768.0000 - val_mae: 243450.3750\n",
      "Epoch 42/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 72026521600.0000 - mae: 231897.9375 - val_loss: 78321426432.0000 - val_mae: 243307.6406\n",
      "Epoch 43/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 76430376960.0000 - mae: 239122.0781 - val_loss: 78358077440.0000 - val_mae: 243371.7344\n",
      "Epoch 44/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 76408807424.0000 - mae: 239900.1406 - val_loss: 78376976384.0000 - val_mae: 243425.5000\n",
      "Epoch 45/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 76141477888.0000 - mae: 238425.2969 - val_loss: 78299160576.0000 - val_mae: 243259.2031\n",
      "Epoch 46/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 76443205632.0000 - mae: 239308.1094 - val_loss: 78343602176.0000 - val_mae: 243350.6250\n",
      "Epoch 47/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75724201984.0000 - mae: 238109.4375 - val_loss: 78395711488.0000 - val_mae: 243474.4531\n",
      "Epoch 48/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 72992186368.0000 - mae: 232685.0469 - val_loss: 78324981760.0000 - val_mae: 243214.7344\n",
      "Epoch 49/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75827134464.0000 - mae: 239531.5469 - val_loss: 78386757632.0000 - val_mae: 243403.6406\n",
      "Epoch 50/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 73046401024.0000 - mae: 231226.4219 - val_loss: 78318280704.0000 - val_mae: 243260.9844\n",
      "Epoch 51/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75217600512.0000 - mae: 236158.6719 - val_loss: 78314037248.0000 - val_mae: 243242.1406\n",
      "Epoch 52/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75663269888.0000 - mae: 236894.5938 - val_loss: 78307508224.0000 - val_mae: 243168.9219\n",
      "Epoch 53/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75950104576.0000 - mae: 239812.2500 - val_loss: 78345240576.0000 - val_mae: 243329.6562\n",
      "Epoch 54/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74651213824.0000 - mae: 235854.9531 - val_loss: 78323163136.0000 - val_mae: 243249.0625\n",
      "Epoch 55/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 76069322752.0000 - mae: 238373.8906 - val_loss: 78335975424.0000 - val_mae: 243303.4219\n",
      "Epoch 56/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73546104832.0000 - mae: 234884.9844 - val_loss: 78329069568.0000 - val_mae: 243209.2344\n",
      "Epoch 57/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74667655168.0000 - mae: 236843.4375 - val_loss: 78294753280.0000 - val_mae: 243176.6250\n",
      "Epoch 58/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75755675648.0000 - mae: 237028.8125 - val_loss: 78295711744.0000 - val_mae: 243107.6250\n",
      "Epoch 59/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 73392701440.0000 - mae: 233763.6250 - val_loss: 78272487424.0000 - val_mae: 242893.9844\n",
      "Epoch 60/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 72110718976.0000 - mae: 231459.6250 - val_loss: 78298456064.0000 - val_mae: 243232.7188\n",
      "Epoch 61/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74596663296.0000 - mae: 234544.6562 - val_loss: 78355742720.0000 - val_mae: 243366.9531\n",
      "Epoch 62/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74816667648.0000 - mae: 238171.1250 - val_loss: 78376157184.0000 - val_mae: 243388.7812\n",
      "Epoch 63/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75680079872.0000 - mae: 236383.8906 - val_loss: 78302945280.0000 - val_mae: 243152.9062\n",
      "Epoch 64/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73043755008.0000 - mae: 231220.9375 - val_loss: 78334681088.0000 - val_mae: 243303.8594\n",
      "Epoch 65/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 76746424320.0000 - mae: 241774.3125 - val_loss: 78332534784.0000 - val_mae: 243247.1250\n",
      "Epoch 66/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75682349056.0000 - mae: 238077.9062 - val_loss: 78329528320.0000 - val_mae: 243274.3594\n",
      "Epoch 67/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75413864448.0000 - mae: 238076.0156 - val_loss: 78341545984.0000 - val_mae: 243290.7188\n",
      "Epoch 68/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74166820864.0000 - mae: 235023.0781 - val_loss: 78315200512.0000 - val_mae: 243229.8438\n",
      "Epoch 69/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75841568768.0000 - mae: 239800.0469 - val_loss: 78316797952.0000 - val_mae: 243279.1875\n",
      "Epoch 70/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 76568256512.0000 - mae: 240035.3438 - val_loss: 78330306560.0000 - val_mae: 243281.9375\n",
      "Epoch 71/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75182735360.0000 - mae: 236509.5312 - val_loss: 78365130752.0000 - val_mae: 243408.1250\n",
      "Epoch 72/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 77304381440.0000 - mae: 240440.1562 - val_loss: 78327111680.0000 - val_mae: 243290.5938\n",
      "Epoch 73/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 77123461120.0000 - mae: 239585.0938 - val_loss: 78393729024.0000 - val_mae: 243462.9219\n",
      "Epoch 74/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 77199671296.0000 - mae: 242102.9062 - val_loss: 78334246912.0000 - val_mae: 243292.4375\n",
      "Epoch 75/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 73176014848.0000 - mae: 232841.8281 - val_loss: 78338596864.0000 - val_mae: 243280.4219\n",
      "Epoch 76/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 76082880512.0000 - mae: 238560.8281 - val_loss: 78276968448.0000 - val_mae: 243074.3438\n",
      "Epoch 77/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 76752920576.0000 - mae: 240245.2812 - val_loss: 78283939840.0000 - val_mae: 243067.9844\n",
      "Epoch 78/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 74611630080.0000 - mae: 235871.2969 - val_loss: 78310473728.0000 - val_mae: 243210.2188\n",
      "Epoch 79/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73403768832.0000 - mae: 234811.1250 - val_loss: 78320467968.0000 - val_mae: 243198.6406\n",
      "Epoch 80/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 73209667584.0000 - mae: 233862.4844 - val_loss: 78372708352.0000 - val_mae: 243401.2656\n",
      "Epoch 81/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74842529792.0000 - mae: 236147.5938 - val_loss: 78298693632.0000 - val_mae: 243165.3438\n",
      "Epoch 82/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 73567739904.0000 - mae: 234224.3750 - val_loss: 78292574208.0000 - val_mae: 243090.4531\n",
      "Epoch 83/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75885445120.0000 - mae: 238231.2812 - val_loss: 78335737856.0000 - val_mae: 243300.8125\n",
      "Epoch 84/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74701922304.0000 - mae: 236629.4844 - val_loss: 78312144896.0000 - val_mae: 243208.0781\n",
      "Epoch 85/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 77479452672.0000 - mae: 240476.1562 - val_loss: 78300004352.0000 - val_mae: 243191.7969\n",
      "Epoch 86/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 76770508800.0000 - mae: 240238.2500 - val_loss: 78347943936.0000 - val_mae: 243276.7031\n",
      "Epoch 87/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 79069511680.0000 - mae: 242855.9531 - val_loss: 78350753792.0000 - val_mae: 243288.8750\n",
      "Epoch 88/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 77003857920.0000 - mae: 240574.1094 - val_loss: 78307368960.0000 - val_mae: 243201.1250\n",
      "Epoch 89/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 76324061184.0000 - mae: 238376.2344 - val_loss: 78280007680.0000 - val_mae: 243124.0781\n",
      "Epoch 90/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75622170624.0000 - mae: 239388.6875 - val_loss: 78410579968.0000 - val_mae: 243480.2188\n",
      "Epoch 91/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 76447531008.0000 - mae: 239179.8125 - val_loss: 78307606528.0000 - val_mae: 243166.1875\n",
      "Epoch 92/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 74533150720.0000 - mae: 236435.8281 - val_loss: 78321426432.0000 - val_mae: 243251.1406\n",
      "Epoch 93/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73325314048.0000 - mae: 233408.7969 - val_loss: 78314045440.0000 - val_mae: 243174.5625\n",
      "Epoch 94/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75100413952.0000 - mae: 237108.6562 - val_loss: 78271619072.0000 - val_mae: 242976.3438\n",
      "Epoch 95/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 76738060288.0000 - mae: 239602.6719 - val_loss: 78311153664.0000 - val_mae: 243243.7344\n",
      "Epoch 96/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 73599664128.0000 - mae: 234473.0469 - val_loss: 78298619904.0000 - val_mae: 243191.0781\n",
      "Epoch 97/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 76756156416.0000 - mae: 239889.7344 - val_loss: 78339375104.0000 - val_mae: 243289.1875\n",
      "Epoch 98/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 73537806336.0000 - mae: 235069.0000 - val_loss: 78322589696.0000 - val_mae: 243249.9219\n",
      "Epoch 99/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 73794338816.0000 - mae: 234502.7188 - val_loss: 78275100672.0000 - val_mae: 243136.0781\n",
      "Epoch 100/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 76491816960.0000 - mae: 240240.9844 - val_loss: 78332428288.0000 - val_mae: 243253.0781\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 959us/step - loss: 75051671552.0000 - mae: 236983.7188\n",
      "Test MAE: 243253.078125\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "[[522103.25]\n",
      " [548555.2 ]\n",
      " [486650.38]\n",
      " [539718.1 ]\n",
      " [553381.7 ]\n",
      " [520781.25]\n",
      " [522833.2 ]\n",
      " [578023.2 ]\n",
      " [545993.94]\n",
      " [576744.44]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load dataset (assuming you have a CSV file)\n",
    "df=pd.read_csv('HousePrice.csv')\n",
    "\n",
    "# Define features and target\n",
    "X = df.drop(columns=[\"Id\", \"Price\"])  # Exclude ID and target\n",
    "Y = df[\"Price\"].values\n",
    "\n",
    "# Identify categorical and numerical features\n",
    "categorical_features = [\"Location\", \"Condition\", \"Garage\"]\n",
    "numerical_features = [\"Area\", \"Bedrooms\", \"Bathrooms\", \"Floors\", \"YearBuilt\"]\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numerical_features),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Transform features\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)\n",
    "\n",
    "# Build the neural network model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(32, activation='relu'),\n",
    "    keras.layers.Dense(1)  # Output layer for regression\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, Y_train, epochs=100, batch_size=32, validation_data=(X_test, Y_test))\n",
    "\n",
    "# Evaluate model\n",
    "loss, mae = model.evaluate(X_test, Y_test)\n",
    "print(f\"Test MAE: {mae}\")\n",
    "\n",
    "# Predict\n",
    "predictions = model.predict(X_test)\n",
    "print(predictions[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "5dcaf8bf-6326-4b36-bd3b-24f924eec02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saich\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 377304547328.0000 - mae: 547317.6875 - val_loss: 349373104128.0000 - val_mae: 526897.3125\n",
      "Epoch 2/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 372880932864.0000 - mae: 541935.3750 - val_loss: 349364092928.0000 - val_mae: 526888.8125\n",
      "Epoch 3/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 370742919168.0000 - mae: 539350.3125 - val_loss: 349340762112.0000 - val_mae: 526866.6250\n",
      "Epoch 4/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 370618630144.0000 - mae: 540793.4375 - val_loss: 349290168320.0000 - val_mae: 526818.8125\n",
      "Epoch 5/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 355250438144.0000 - mae: 529960.0000 - val_loss: 349196943360.0000 - val_mae: 526730.5625\n",
      "Epoch 6/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 366382186496.0000 - mae: 537306.2500 - val_loss: 349045817344.0000 - val_mae: 526587.3750\n",
      "Epoch 7/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 354810232832.0000 - mae: 529721.8125 - val_loss: 348825255936.0000 - val_mae: 526378.5000\n",
      "Epoch 8/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 377157287936.0000 - mae: 547403.4375 - val_loss: 348521496576.0000 - val_mae: 526090.5000\n",
      "Epoch 9/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 373059846144.0000 - mae: 542531.3750 - val_loss: 348123791360.0000 - val_mae: 525713.3750\n",
      "Epoch 10/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 368843063296.0000 - mae: 538516.0625 - val_loss: 347621326848.0000 - val_mae: 525236.6250\n",
      "Epoch 11/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 357717508096.0000 - mae: 530707.1875 - val_loss: 347010859008.0000 - val_mae: 524656.1875\n",
      "Epoch 12/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 368075210752.0000 - mae: 541139.8125 - val_loss: 346275053568.0000 - val_mae: 523955.9375\n",
      "Epoch 13/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 366352531456.0000 - mae: 539519.0625 - val_loss: 345408176128.0000 - val_mae: 523130.5000\n",
      "Epoch 14/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 358914031616.0000 - mae: 533578.6250 - val_loss: 344412192768.0000 - val_mae: 522179.5938\n",
      "Epoch 15/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 367552856064.0000 - mae: 539720.4375 - val_loss: 343258464256.0000 - val_mae: 521076.6875\n",
      "Epoch 16/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 364205899776.0000 - mae: 536142.9375 - val_loss: 341961408512.0000 - val_mae: 519831.4375\n",
      "Epoch 17/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 362459987968.0000 - mae: 534587.0625 - val_loss: 340509360128.0000 - val_mae: 518437.1562\n",
      "Epoch 18/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 365766180864.0000 - mae: 534692.7500 - val_loss: 338887016448.0000 - val_mae: 516872.4062\n",
      "Epoch 19/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 367873097728.0000 - mae: 539149.8125 - val_loss: 337107812352.0000 - val_mae: 515152.0625\n",
      "Epoch 20/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 363973279744.0000 - mae: 535680.6875 - val_loss: 335151038464.0000 - val_mae: 513254.6562\n",
      "Epoch 21/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 349236428800.0000 - mae: 523040.9688 - val_loss: 333030981632.0000 - val_mae: 511189.5938\n",
      "Epoch 22/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 362355752960.0000 - mae: 535648.3125 - val_loss: 330723885056.0000 - val_mae: 508931.2500\n",
      "Epoch 23/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 337747345408.0000 - mae: 508698.3438 - val_loss: 328264318976.0000 - val_mae: 506514.0000\n",
      "Epoch 24/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 345035636736.0000 - mae: 519008.5000 - val_loss: 325589368832.0000 - val_mae: 503872.0625\n",
      "Epoch 25/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 346007568384.0000 - mae: 517835.5312 - val_loss: 322755559424.0000 - val_mae: 501056.8438\n",
      "Epoch 26/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 348888662016.0000 - mae: 523543.5312 - val_loss: 319737757696.0000 - val_mae: 498042.3438\n",
      "Epoch 27/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 314349617152.0000 - mae: 488074.5000 - val_loss: 316572565504.0000 - val_mae: 494862.2500\n",
      "Epoch 28/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 331583946752.0000 - mae: 502479.3438 - val_loss: 313188352000.0000 - val_mae: 491436.9375\n",
      "Epoch 29/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 331479908352.0000 - mae: 506466.1562 - val_loss: 309659860992.0000 - val_mae: 487841.8438\n",
      "Epoch 30/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 319002968064.0000 - mae: 493302.4375 - val_loss: 305952161792.0000 - val_mae: 484032.8438\n",
      "Epoch 31/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 325041651712.0000 - mae: 497210.3438 - val_loss: 302090911744.0000 - val_mae: 480033.5000\n",
      "Epoch 32/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 314721042432.0000 - mae: 488392.1250 - val_loss: 298035019776.0000 - val_mae: 475810.8125\n",
      "Epoch 33/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 316397617152.0000 - mae: 486860.2188 - val_loss: 293853822976.0000 - val_mae: 471434.0000\n",
      "Epoch 34/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 306234261504.0000 - mae: 481275.5000 - val_loss: 289495384064.0000 - val_mae: 466846.5625\n",
      "Epoch 35/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 298309353472.0000 - mae: 469998.2188 - val_loss: 285029171200.0000 - val_mae: 462164.0938\n",
      "Epoch 36/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 296597356544.0000 - mae: 470366.3750 - val_loss: 280391909376.0000 - val_mae: 457334.7500\n",
      "Epoch 37/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 305577164800.0000 - mae: 477219.1875 - val_loss: 275592740864.0000 - val_mae: 452331.1875\n",
      "Epoch 38/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 288911785984.0000 - mae: 465043.7500 - val_loss: 270793670656.0000 - val_mae: 447319.4375\n",
      "Epoch 39/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 274102681600.0000 - mae: 444579.5625 - val_loss: 265770713088.0000 - val_mae: 442037.0625\n",
      "Epoch 40/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 269680328704.0000 - mae: 443222.4688 - val_loss: 260653842432.0000 - val_mae: 436676.0938\n",
      "Epoch 41/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 275058589696.0000 - mae: 449070.5312 - val_loss: 255442321408.0000 - val_mae: 431154.0000\n",
      "Epoch 42/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 261280137216.0000 - mae: 431679.0000 - val_loss: 250193035264.0000 - val_mae: 425565.6875\n",
      "Epoch 43/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 250448429056.0000 - mae: 423516.6250 - val_loss: 244830175232.0000 - val_mae: 419829.5000\n",
      "Epoch 44/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 249860145152.0000 - mae: 421902.3438 - val_loss: 239417819136.0000 - val_mae: 414067.0625\n",
      "Epoch 45/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 260504207360.0000 - mae: 433085.7812 - val_loss: 233873080320.0000 - val_mae: 408116.0312\n",
      "Epoch 46/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 249611124736.0000 - mae: 420212.0312 - val_loss: 228376608768.0000 - val_mae: 402166.0625\n",
      "Epoch 47/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 235169431552.0000 - mae: 404678.4062 - val_loss: 222816944128.0000 - val_mae: 396120.4375\n",
      "Epoch 48/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 242952617984.0000 - mae: 414473.1562 - val_loss: 217217335296.0000 - val_mae: 390017.9062\n",
      "Epoch 49/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 241379770368.0000 - mae: 415625.9688 - val_loss: 211623411712.0000 - val_mae: 383899.8438\n",
      "Epoch 50/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 218537918464.0000 - mae: 387887.6562 - val_loss: 206033633280.0000 - val_mae: 377819.9688\n",
      "Epoch 51/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 211606618112.0000 - mae: 382762.8125 - val_loss: 200491237376.0000 - val_mae: 371723.0000\n",
      "Epoch 52/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 212837220352.0000 - mae: 383224.5625 - val_loss: 194915254272.0000 - val_mae: 365534.4062\n",
      "Epoch 53/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 209107238912.0000 - mae: 382784.8750 - val_loss: 189355622400.0000 - val_mae: 359470.2188\n",
      "Epoch 54/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 209742413824.0000 - mae: 382855.0000 - val_loss: 183860232192.0000 - val_mae: 353641.3125\n",
      "Epoch 55/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 197091737600.0000 - mae: 365807.7188 - val_loss: 178465767424.0000 - val_mae: 347979.9062\n",
      "Epoch 56/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 194802712576.0000 - mae: 364166.4062 - val_loss: 173066387456.0000 - val_mae: 342383.8125\n",
      "Epoch 57/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 181437890560.0000 - mae: 348502.3750 - val_loss: 167861321728.0000 - val_mae: 336901.4375\n",
      "Epoch 58/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 180872151040.0000 - mae: 350129.8438 - val_loss: 162648064000.0000 - val_mae: 331509.5625\n",
      "Epoch 59/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 171070013440.0000 - mae: 336535.0000 - val_loss: 157602332672.0000 - val_mae: 326271.5000\n",
      "Epoch 60/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 171387518976.0000 - mae: 343961.8125 - val_loss: 152602836992.0000 - val_mae: 320968.9062\n",
      "Epoch 61/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 157746741248.0000 - mae: 324520.9375 - val_loss: 147750076416.0000 - val_mae: 315824.4375\n",
      "Epoch 62/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 164160258048.0000 - mae: 333350.8750 - val_loss: 142958837760.0000 - val_mae: 310760.5625\n",
      "Epoch 63/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 154000310272.0000 - mae: 322057.0938 - val_loss: 138360897536.0000 - val_mae: 305939.8438\n",
      "Epoch 64/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 138724245504.0000 - mae: 301463.5312 - val_loss: 133970419712.0000 - val_mae: 301328.3438\n",
      "Epoch 65/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 145946116096.0000 - mae: 313030.1250 - val_loss: 129576574976.0000 - val_mae: 296710.0625\n",
      "Epoch 66/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 140049530880.0000 - mae: 306901.8750 - val_loss: 125454032896.0000 - val_mae: 292366.0938\n",
      "Epoch 67/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 135661756416.0000 - mae: 301328.2188 - val_loss: 121430392832.0000 - val_mae: 288170.6875\n",
      "Epoch 68/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 126902632448.0000 - mae: 292024.1250 - val_loss: 117569929216.0000 - val_mae: 284124.7500\n",
      "Epoch 69/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 132862132224.0000 - mae: 300245.2188 - val_loss: 113901985792.0000 - val_mae: 280328.3438\n",
      "Epoch 70/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 128811868160.0000 - mae: 296154.2500 - val_loss: 110386872320.0000 - val_mae: 276678.9375\n",
      "Epoch 71/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 115583901696.0000 - mae: 279383.8438 - val_loss: 107106861056.0000 - val_mae: 273137.1562\n",
      "Epoch 72/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 121834455040.0000 - mae: 291308.0000 - val_loss: 103932772352.0000 - val_mae: 269624.8125\n",
      "Epoch 73/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 111807463424.0000 - mae: 277127.1250 - val_loss: 100967628800.0000 - val_mae: 266334.6562\n",
      "Epoch 74/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 115167576064.0000 - mae: 283420.2188 - val_loss: 98157379584.0000 - val_mae: 263096.5625\n",
      "Epoch 75/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 108830597120.0000 - mae: 275864.1875 - val_loss: 95562547200.0000 - val_mae: 260060.8750\n",
      "Epoch 76/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 106806509568.0000 - mae: 273079.4062 - val_loss: 93125353472.0000 - val_mae: 257172.7812\n",
      "Epoch 77/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 101974204416.0000 - mae: 267831.5312 - val_loss: 90835017728.0000 - val_mae: 254414.9219\n",
      "Epoch 78/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 96912916480.0000 - mae: 258138.8281 - val_loss: 88748318720.0000 - val_mae: 251884.5469\n",
      "Epoch 79/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 95076958208.0000 - mae: 258678.0938 - val_loss: 86816718848.0000 - val_mae: 249615.9531\n",
      "Epoch 80/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 90853220352.0000 - mae: 252397.1875 - val_loss: 85035253760.0000 - val_mae: 247585.0000\n",
      "Epoch 81/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 92293750784.0000 - mae: 256570.1562 - val_loss: 83392708608.0000 - val_mae: 245732.4062\n",
      "Epoch 82/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 92567429120.0000 - mae: 258379.1094 - val_loss: 81902256128.0000 - val_mae: 244113.4219\n",
      "Epoch 83/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 90967171072.0000 - mae: 257466.9531 - val_loss: 80547397632.0000 - val_mae: 242660.1250\n",
      "Epoch 84/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 89098715136.0000 - mae: 253668.5625 - val_loss: 79359991808.0000 - val_mae: 241330.5312\n",
      "Epoch 85/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 90543841280.0000 - mae: 257559.2188 - val_loss: 78233288704.0000 - val_mae: 240013.5938\n",
      "Epoch 86/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 82799476736.0000 - mae: 245915.2188 - val_loss: 77295452160.0000 - val_mae: 238865.4531\n",
      "Epoch 87/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 83741523968.0000 - mae: 247493.2656 - val_loss: 76400017408.0000 - val_mae: 237736.0781\n",
      "Epoch 88/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 79093596160.0000 - mae: 239232.6406 - val_loss: 75689705472.0000 - val_mae: 236807.9062\n",
      "Epoch 89/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 79282364416.0000 - mae: 242825.6719 - val_loss: 74971258880.0000 - val_mae: 235806.9062\n",
      "Epoch 90/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 80849403904.0000 - mae: 243451.2969 - val_loss: 74383007744.0000 - val_mae: 234987.7812\n",
      "Epoch 91/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 82956001280.0000 - mae: 248607.4844 - val_loss: 73879339008.0000 - val_mae: 234229.4062\n",
      "Epoch 92/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 82162974720.0000 - mae: 247035.0156 - val_loss: 73448701952.0000 - val_mae: 233549.2500\n",
      "Epoch 93/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 77005832192.0000 - mae: 236901.5469 - val_loss: 73084739584.0000 - val_mae: 232941.8438\n",
      "Epoch 94/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 81481367552.0000 - mae: 245087.5156 - val_loss: 72750727168.0000 - val_mae: 232331.4688\n",
      "Epoch 95/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 77028687872.0000 - mae: 237087.9219 - val_loss: 72499806208.0000 - val_mae: 231830.4062\n",
      "Epoch 96/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 78056857600.0000 - mae: 238770.8594 - val_loss: 72291729408.0000 - val_mae: 231371.6562\n",
      "Epoch 97/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 79016353792.0000 - mae: 243565.6562 - val_loss: 72114798592.0000 - val_mae: 230953.0781\n",
      "Epoch 98/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 79373197312.0000 - mae: 243337.1406 - val_loss: 71954317312.0000 - val_mae: 230535.0469\n",
      "Epoch 99/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 77051682816.0000 - mae: 239998.7656 - val_loss: 71848902656.0000 - val_mae: 230245.2500\n",
      "Epoch 100/100\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75628830720.0000 - mae: 236711.7500 - val_loss: 71755374592.0000 - val_mae: 229996.4062\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "R² Score: -0.0291\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Load the dataset\n",
    " # Replace with your actual dataset\n",
    "\n",
    "# Define features and target variable\n",
    "X = df.drop(columns=['Id', 'Price'])  # Features\n",
    "y = df['Price']  # Target variable\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_cols = ['Location', 'Condition', 'Garage']\n",
    "numerical_cols = ['Area', 'Bedrooms', 'Bathrooms', 'Floors', 'YearBuilt']\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", StandardScaler(), numerical_cols),\n",
    "    (\"cat\", OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "])\n",
    "\n",
    "# Transform the dataset\n",
    "X_transformed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the Neural Network Model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    keras.layers.Dense(32, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='linear')  # Linear activation for regression\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate R² Score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"R² Score: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611757d4-8e32-4bb8-8bc3-467b68c23c7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
